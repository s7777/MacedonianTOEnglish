{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_32vuc5A-Vlk",
        "outputId": "f3380473-fbfc-43e4-aeaf-2a74b7a76b8b"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "\tlines = doc.strip().split('\\n')\n",
        "\tpairs = [line.split('\\t') for line in  lines]\n",
        "\treturn pairs\n",
        "\n",
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor pair in lines:\n",
        "\t\tclean_pair = list()\n",
        "\t\tfor line in pair:\n",
        "\n",
        "\t\t\tline = line.split()\n",
        "\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tline = [word.translate(table) for word in line]\n",
        "\n",
        "\t\t\t# store as string\n",
        "\t\t\tclean_pair.append(' '.join(line))\n",
        "\t\tcleaned.append(clean_pair)\n",
        "\treturn array(cleaned)\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "filename = '/content/mkd.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into english-macedonian pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, 'english-mkd.pkl')\n",
        "# spot check\n",
        "for i in range(100):\n",
        "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-mkd.pkl\n",
            "[Go] => [Оди]\n",
            "[Hi] => [Здраво]\n",
            "[Hi] => [Кај си бе]\n",
            "[Run] => [Бегај]\n",
            "[Run] => [Трчај]\n",
            "[Wow] => [Леле]\n",
            "[Wow] => [Боже]\n",
            "[Wow] => [Ајде]\n",
            "[Help] => [Помош]\n",
            "[Jump] => [Скокај]\n",
            "[Go on] => [Продолжи]\n",
            "[Hello] => [Здраво]\n",
            "[I see] => [Сфаќам]\n",
            "[I try] => [Се трудам]\n",
            "[I won] => [Победив]\n",
            "[Oh no] => [О не]\n",
            "[Smile] => [Насмеј се]\n",
            "[Freeze] => [Не се мрдај]\n",
            "[Freeze] => [Стој]\n",
            "[Get up] => [Станувај]\n",
            "[Go now] => [Сега оди]\n",
            "[He ran] => [Тој трчаше]\n",
            "[Hop in] => [Влегувај]\n",
            "[Hug me] => [Гушни ме]\n",
            "[I fell] => [Паднав]\n",
            "[I know] => [Знам де]\n",
            "[I know] => [Знам]\n",
            "[I lost] => [Изгубив]\n",
            "[I quit] => [Се откажав]\n",
            "[Im 19] => [Имам деветнаесет години]\n",
            "[Im OK] => [Добро сум]\n",
            "[Im OK] => [Добар сум]\n",
            "[Im up] => [Станат сум]\n",
            "[Listen] => [Слушај]\n",
            "[No way] => [Не доаѓа во обѕир]\n",
            "[Really] => [Навистина]\n",
            "[Thanks] => [Фала]\n",
            "[We try] => [Се трудиме]\n",
            "[We won] => [Победивме]\n",
            "[Why me] => [Зошто јас]\n",
            "[Why me] => [Зошто мене]\n",
            "[Ask Tom] => [Прашај го Том]\n",
            "[Ask him] => [Прашај го]\n",
            "[Be calm] => [Биди спокоен]\n",
            "[Be cool] => [Биди опуштен]\n",
            "[Be fair] => [Биди праведен]\n",
            "[Be kind] => [Биди љубезен]\n",
            "[Be nice] => [Биди фин]\n",
            "[Beat it] => [Гони се]\n",
            "[Call me] => [Јави ми се]\n",
            "[Call us] => [Јави ни се]\n",
            "[Come in] => [Влези]\n",
            "[Come on] => [Ајде]\n",
            "[Come on] => [Ајде]\n",
            "[Drop it] => [Пушти го]\n",
            "[Get Tom] => [Викни го Том]\n",
            "[Get out] => [Излегувај]\n",
            "[Get out] => [Надвор]\n",
            "[Get out] => [Излегувај]\n",
            "[Go away] => [Оди си бе]\n",
            "[Go away] => [Оди си]\n",
            "[Go home] => [Оди си дома]\n",
            "[Go slow] => [Оди полека]\n",
            "[Goodbye] => [Збогум]\n",
            "[Goodbye] => [Догледање]\n",
            "[Goodbye] => [Пријатно]\n",
            "[Goodbye] => [Довидување]\n",
            "[Hang on] => [Држи се]\n",
            "[Hang on] => [Држи се]\n",
            "[He came] => [Тој дојде]\n",
            "[He runs] => [Тој трча]\n",
            "[Help me] => [Помогни ми]\n",
            "[Help me] => [Помогни ми]\n",
            "[Help us] => [Помогни ни]\n",
            "[Hi Tom] => [Здраво Том]\n",
            "[Hit Tom] => [Удри го Том]\n",
            "[Hold it] => [Држи го]\n",
            "[Hold on] => [Почекај]\n",
            "[Hug Tom] => [Гушни го Том]\n",
            "[Hug Tom] => [Прегрни го Том]\n",
            "[I agree] => [Се согласувам]\n",
            "[I agree] => [Се сложувам]\n",
            "[I tried] => [Уморен сум]\n",
            "[Ill go] => [Ќе одам]\n",
            "[Im Tom] => [Јас сум Том]\n",
            "[Im fat] => [Дебел сум]\n",
            "[Im hit] => [Погоден сум]\n",
            "[Im hot] => [Топло ми е]\n",
            "[Im old] => [Стар сум]\n",
            "[Im old] => [Стара сум]\n",
            "[Im sad] => [Тажен сум]\n",
            "[Im shy] => [Срамежлив сум]\n",
            "[Im wet] => [Мокар сум]\n",
            "[Im wet] => [Воден сум]\n",
            "[Its OK] => [Во ред е]\n",
            "[Its me] => [Јас сум]\n",
            "[Its me] => [Јас сум]\n",
            "[Join us] => [Придружи ни се]\n",
            "[Keep it] => [Чувај го]\n",
            "[Kiss me] => [Бакни ме]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hP5oEiD-2sm",
        "outputId": "7c957406-98ab-4fb5-edf8-e72f2aacccf9"
      },
      "source": [
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('english-mkd.pkl')\n",
        "\n",
        "# reduce dataset size\n",
        "n_sentences = 30000\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "train, test = dataset[:29000], dataset[1000:]\n",
        "# save\n",
        "save_clean_data(dataset, 'english-mkd-both.pkl')\n",
        "save_clean_data(train, 'english-mkd-train.pkl')\n",
        "save_clean_data(test, 'english-mkd-test.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-mkd-both.pkl\n",
            "Saved: english-mkd-train.pkl\n",
            "Saved: english-mkd-test.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN1lcO-P-3x-",
        "outputId": "e8732c77-9b25-478f-a2f9-b1e9ecb55bbe"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-mkd-both.pkl')\n",
        "train = load_clean_sentences('english-mkd-train.pkl')\n",
        "test = load_clean_sentences('english-mkd-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:,0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare  bangla tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Macedonian Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Macedonian Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=200, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 4326\n",
            "English Max Length: 6\n",
            "Macedonian Vocabulary Size: 9102\n",
            "Macedonian Max Length: 10\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 10, 256)           2330112   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 6, 256)            0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 6, 256)            525312    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 6, 4326)           1111782   \n",
            "=================================================================\n",
            "Total params: 4,492,518\n",
            "Trainable params: 4,492,518\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "297/297 - 94s - loss: 4.2503 - val_loss: 3.5971\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.59707, saving model to model.h5\n",
            "Epoch 2/200\n",
            "297/297 - 87s - loss: 3.5235 - val_loss: 3.3762\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.59707 to 3.37619, saving model to model.h5\n",
            "Epoch 3/200\n",
            "297/297 - 88s - loss: 3.3176 - val_loss: 3.1570\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.37619 to 3.15698, saving model to model.h5\n",
            "Epoch 4/200\n",
            "297/297 - 87s - loss: 3.0859 - val_loss: 2.9220\n",
            "\n",
            "Epoch 00004: val_loss improved from 3.15698 to 2.92202, saving model to model.h5\n",
            "Epoch 5/200\n",
            "297/297 - 87s - loss: 2.8581 - val_loss: 2.6943\n",
            "\n",
            "Epoch 00005: val_loss improved from 2.92202 to 2.69433, saving model to model.h5\n",
            "Epoch 6/200\n",
            "297/297 - 87s - loss: 2.6537 - val_loss: 2.4984\n",
            "\n",
            "Epoch 00006: val_loss improved from 2.69433 to 2.49835, saving model to model.h5\n",
            "Epoch 7/200\n",
            "297/297 - 87s - loss: 2.4600 - val_loss: 2.3167\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.49835 to 2.31674, saving model to model.h5\n",
            "Epoch 8/200\n",
            "297/297 - 87s - loss: 2.2781 - val_loss: 2.1505\n",
            "\n",
            "Epoch 00008: val_loss improved from 2.31674 to 2.15051, saving model to model.h5\n",
            "Epoch 9/200\n",
            "297/297 - 87s - loss: 2.1157 - val_loss: 1.9916\n",
            "\n",
            "Epoch 00009: val_loss improved from 2.15051 to 1.99162, saving model to model.h5\n",
            "Epoch 10/200\n",
            "297/297 - 87s - loss: 1.9645 - val_loss: 1.8483\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.99162 to 1.84829, saving model to model.h5\n",
            "Epoch 11/200\n",
            "297/297 - 87s - loss: 1.8214 - val_loss: 1.7187\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.84829 to 1.71866, saving model to model.h5\n",
            "Epoch 12/200\n",
            "297/297 - 87s - loss: 1.6886 - val_loss: 1.5908\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.71866 to 1.59081, saving model to model.h5\n",
            "Epoch 13/200\n",
            "297/297 - 88s - loss: 1.5612 - val_loss: 1.4746\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.59081 to 1.47457, saving model to model.h5\n",
            "Epoch 14/200\n",
            "297/297 - 87s - loss: 1.4418 - val_loss: 1.3596\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.47457 to 1.35964, saving model to model.h5\n",
            "Epoch 15/200\n",
            "297/297 - 89s - loss: 1.3285 - val_loss: 1.2615\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.35964 to 1.26146, saving model to model.h5\n",
            "Epoch 16/200\n",
            "297/297 - 88s - loss: 1.2186 - val_loss: 1.1570\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.26146 to 1.15698, saving model to model.h5\n",
            "Epoch 17/200\n",
            "297/297 - 91s - loss: 1.1162 - val_loss: 1.0604\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.15698 to 1.06038, saving model to model.h5\n",
            "Epoch 18/200\n",
            "297/297 - 90s - loss: 1.0189 - val_loss: 0.9801\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.06038 to 0.98010, saving model to model.h5\n",
            "Epoch 19/200\n",
            "297/297 - 89s - loss: 0.9282 - val_loss: 0.8949\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.98010 to 0.89487, saving model to model.h5\n",
            "Epoch 20/200\n",
            "297/297 - 89s - loss: 0.8447 - val_loss: 0.8242\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.89487 to 0.82424, saving model to model.h5\n",
            "Epoch 21/200\n",
            "297/297 - 93s - loss: 0.7650 - val_loss: 0.7535\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.82424 to 0.75350, saving model to model.h5\n",
            "Epoch 22/200\n",
            "297/297 - 88s - loss: 0.6928 - val_loss: 0.6940\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.75350 to 0.69402, saving model to model.h5\n",
            "Epoch 23/200\n",
            "297/297 - 87s - loss: 0.6291 - val_loss: 0.6418\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.69402 to 0.64176, saving model to model.h5\n",
            "Epoch 24/200\n",
            "297/297 - 87s - loss: 0.5694 - val_loss: 0.5886\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.64176 to 0.58859, saving model to model.h5\n",
            "Epoch 25/200\n",
            "297/297 - 87s - loss: 0.5150 - val_loss: 0.5462\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.58859 to 0.54621, saving model to model.h5\n",
            "Epoch 26/200\n",
            "297/297 - 88s - loss: 0.4676 - val_loss: 0.5081\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.54621 to 0.50814, saving model to model.h5\n",
            "Epoch 27/200\n",
            "297/297 - 88s - loss: 0.4235 - val_loss: 0.4750\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.50814 to 0.47502, saving model to model.h5\n",
            "Epoch 28/200\n",
            "297/297 - 87s - loss: 0.3847 - val_loss: 0.4460\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.47502 to 0.44595, saving model to model.h5\n",
            "Epoch 29/200\n",
            "297/297 - 86s - loss: 0.3493 - val_loss: 0.4134\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.44595 to 0.41338, saving model to model.h5\n",
            "Epoch 30/200\n",
            "297/297 - 86s - loss: 0.3152 - val_loss: 0.3866\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.41338 to 0.38659, saving model to model.h5\n",
            "Epoch 31/200\n",
            "297/297 - 87s - loss: 0.2879 - val_loss: 0.3679\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.38659 to 0.36787, saving model to model.h5\n",
            "Epoch 32/200\n",
            "297/297 - 86s - loss: 0.2620 - val_loss: 0.3445\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.36787 to 0.34447, saving model to model.h5\n",
            "Epoch 33/200\n",
            "297/297 - 86s - loss: 0.2386 - val_loss: 0.3253\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.34447 to 0.32528, saving model to model.h5\n",
            "Epoch 34/200\n",
            "297/297 - 86s - loss: 0.2165 - val_loss: 0.3060\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.32528 to 0.30597, saving model to model.h5\n",
            "Epoch 35/200\n",
            "297/297 - 87s - loss: 0.1973 - val_loss: 0.2927\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.30597 to 0.29273, saving model to model.h5\n",
            "Epoch 36/200\n",
            "297/297 - 87s - loss: 0.1828 - val_loss: 0.2825\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.29273 to 0.28247, saving model to model.h5\n",
            "Epoch 37/200\n",
            "297/297 - 87s - loss: 0.1665 - val_loss: 0.2714\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.28247 to 0.27138, saving model to model.h5\n",
            "Epoch 38/200\n",
            "297/297 - 88s - loss: 0.1529 - val_loss: 0.2596\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.27138 to 0.25955, saving model to model.h5\n",
            "Epoch 39/200\n",
            "297/297 - 88s - loss: 0.1386 - val_loss: 0.2512\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.25955 to 0.25117, saving model to model.h5\n",
            "Epoch 40/200\n",
            "297/297 - 87s - loss: 0.1293 - val_loss: 0.2449\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.25117 to 0.24491, saving model to model.h5\n",
            "Epoch 41/200\n",
            "297/297 - 88s - loss: 0.1208 - val_loss: 0.2382\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.24491 to 0.23817, saving model to model.h5\n",
            "Epoch 42/200\n",
            "297/297 - 89s - loss: 0.1144 - val_loss: 0.2325\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.23817 to 0.23251, saving model to model.h5\n",
            "Epoch 43/200\n",
            "297/297 - 88s - loss: 0.1044 - val_loss: 0.2267\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.23251 to 0.22666, saving model to model.h5\n",
            "Epoch 44/200\n",
            "297/297 - 87s - loss: 0.0986 - val_loss: 0.2256\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.22666 to 0.22559, saving model to model.h5\n",
            "Epoch 45/200\n",
            "297/297 - 87s - loss: 0.0952 - val_loss: 0.2224\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.22559 to 0.22242, saving model to model.h5\n",
            "Epoch 46/200\n",
            "297/297 - 88s - loss: 0.0902 - val_loss: 0.2190\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.22242 to 0.21898, saving model to model.h5\n",
            "Epoch 47/200\n",
            "297/297 - 88s - loss: 0.0850 - val_loss: 0.2219\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.21898\n",
            "Epoch 48/200\n",
            "297/297 - 88s - loss: 0.0823 - val_loss: 0.2142\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.21898 to 0.21423, saving model to model.h5\n",
            "Epoch 49/200\n",
            "297/297 - 87s - loss: 0.0801 - val_loss: 0.2114\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.21423 to 0.21142, saving model to model.h5\n",
            "Epoch 50/200\n",
            "297/297 - 87s - loss: 0.0753 - val_loss: 0.2097\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.21142 to 0.20969, saving model to model.h5\n",
            "Epoch 51/200\n",
            "297/297 - 87s - loss: 0.0730 - val_loss: 0.2059\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.20969 to 0.20591, saving model to model.h5\n",
            "Epoch 52/200\n",
            "297/297 - 88s - loss: 0.0683 - val_loss: 0.2062\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.20591\n",
            "Epoch 53/200\n",
            "297/297 - 89s - loss: 0.0681 - val_loss: 0.2058\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.20591 to 0.20581, saving model to model.h5\n",
            "Epoch 54/200\n",
            "297/297 - 88s - loss: 0.0656 - val_loss: 0.2071\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.20581\n",
            "Epoch 55/200\n",
            "297/297 - 88s - loss: 0.0676 - val_loss: 0.2101\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.20581\n",
            "Epoch 56/200\n",
            "297/297 - 87s - loss: 0.0684 - val_loss: 0.2056\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.20581 to 0.20563, saving model to model.h5\n",
            "Epoch 57/200\n",
            "297/297 - 88s - loss: 0.0642 - val_loss: 0.2061\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.20563\n",
            "Epoch 58/200\n",
            "297/297 - 88s - loss: 0.0618 - val_loss: 0.2052\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.20563 to 0.20518, saving model to model.h5\n",
            "Epoch 59/200\n",
            "297/297 - 89s - loss: 0.0604 - val_loss: 0.2012\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.20518 to 0.20123, saving model to model.h5\n",
            "Epoch 60/200\n",
            "297/297 - 88s - loss: 0.0571 - val_loss: 0.1996\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.20123 to 0.19959, saving model to model.h5\n",
            "Epoch 61/200\n",
            "297/297 - 89s - loss: 0.0549 - val_loss: 0.1997\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.19959\n",
            "Epoch 62/200\n",
            "297/297 - 89s - loss: 0.0541 - val_loss: 0.2009\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.19959\n",
            "Epoch 63/200\n",
            "297/297 - 88s - loss: 0.0544 - val_loss: 0.2006\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.19959\n",
            "Epoch 64/200\n",
            "297/297 - 87s - loss: 0.0554 - val_loss: 0.2027\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.19959\n",
            "Epoch 65/200\n",
            "297/297 - 88s - loss: 0.0567 - val_loss: 0.2021\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.19959\n",
            "Epoch 66/200\n",
            "297/297 - 88s - loss: 0.0561 - val_loss: 0.2069\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.19959\n",
            "Epoch 67/200\n",
            "297/297 - 88s - loss: 0.0568 - val_loss: 0.2048\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.19959\n",
            "Epoch 68/200\n",
            "297/297 - 87s - loss: 0.0557 - val_loss: 0.2016\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.19959\n",
            "Epoch 69/200\n",
            "297/297 - 87s - loss: 0.0534 - val_loss: 0.2003\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.19959\n",
            "Epoch 70/200\n",
            "297/297 - 87s - loss: 0.0506 - val_loss: 0.1991\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.19959 to 0.19905, saving model to model.h5\n",
            "Epoch 71/200\n",
            "297/297 - 87s - loss: 0.0493 - val_loss: 0.1990\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.19905 to 0.19904, saving model to model.h5\n",
            "Epoch 72/200\n",
            "297/297 - 86s - loss: 0.0495 - val_loss: 0.2002\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.19904\n",
            "Epoch 73/200\n",
            "297/297 - 87s - loss: 0.0478 - val_loss: 0.1995\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.19904\n",
            "Epoch 74/200\n",
            "297/297 - 87s - loss: 0.0473 - val_loss: 0.1989\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.19904 to 0.19895, saving model to model.h5\n",
            "Epoch 75/200\n",
            "297/297 - 86s - loss: 0.0488 - val_loss: 0.2009\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.19895\n",
            "Epoch 76/200\n",
            "297/297 - 87s - loss: 0.0512 - val_loss: 0.2035\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.19895\n",
            "Epoch 77/200\n",
            "297/297 - 88s - loss: 0.0561 - val_loss: 0.2106\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.19895\n",
            "Epoch 78/200\n",
            "297/297 - 87s - loss: 0.0546 - val_loss: 0.2048\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.19895\n",
            "Epoch 79/200\n",
            "297/297 - 87s - loss: 0.0506 - val_loss: 0.2005\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.19895\n",
            "Epoch 80/200\n",
            "297/297 - 87s - loss: 0.0464 - val_loss: 0.1978\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.19895 to 0.19780, saving model to model.h5\n",
            "Epoch 81/200\n",
            "297/297 - 87s - loss: 0.0449 - val_loss: 0.1996\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.19780\n",
            "Epoch 82/200\n",
            "297/297 - 86s - loss: 0.0439 - val_loss: 0.1991\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.19780\n",
            "Epoch 83/200\n",
            "297/297 - 87s - loss: 0.0436 - val_loss: 0.1979\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.19780\n",
            "Epoch 84/200\n",
            "297/297 - 87s - loss: 0.0442 - val_loss: 0.1992\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.19780\n",
            "Epoch 85/200\n",
            "297/297 - 87s - loss: 0.0467 - val_loss: 0.2109\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.19780\n",
            "Epoch 86/200\n",
            "297/297 - 86s - loss: 0.0558 - val_loss: 0.2135\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.19780\n",
            "Epoch 87/200\n",
            "297/297 - 86s - loss: 0.0545 - val_loss: 0.2082\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.19780\n",
            "Epoch 88/200\n",
            "297/297 - 86s - loss: 0.0487 - val_loss: 0.2023\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.19780\n",
            "Epoch 89/200\n",
            "297/297 - 86s - loss: 0.0442 - val_loss: 0.1979\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.19780\n",
            "Epoch 90/200\n",
            "297/297 - 87s - loss: 0.0423 - val_loss: 0.1984\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.19780\n",
            "Epoch 91/200\n",
            "297/297 - 87s - loss: 0.0411 - val_loss: 0.1994\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.19780\n",
            "Epoch 92/200\n",
            "297/297 - 87s - loss: 0.0414 - val_loss: 0.1994\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.19780\n",
            "Epoch 93/200\n",
            "297/297 - 87s - loss: 0.0414 - val_loss: 0.2009\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.19780\n",
            "Epoch 94/200\n",
            "297/297 - 89s - loss: 0.0422 - val_loss: 0.2004\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.19780\n",
            "Epoch 95/200\n",
            "297/297 - 88s - loss: 0.0420 - val_loss: 0.2020\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.19780\n",
            "Epoch 96/200\n",
            "297/297 - 88s - loss: 0.0430 - val_loss: 0.2015\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.19780\n",
            "Epoch 97/200\n",
            "297/297 - 88s - loss: 0.0445 - val_loss: 0.2100\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.19780\n",
            "Epoch 98/200\n",
            "297/297 - 88s - loss: 0.0555 - val_loss: 0.2146\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.19780\n",
            "Epoch 99/200\n",
            "297/297 - 88s - loss: 0.0566 - val_loss: 0.2143\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.19780\n",
            "Epoch 100/200\n",
            "297/297 - 88s - loss: 0.0512 - val_loss: 0.2071\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.19780\n",
            "Epoch 101/200\n",
            "297/297 - 87s - loss: 0.0442 - val_loss: 0.2015\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.19780\n",
            "Epoch 102/200\n",
            "297/297 - 87s - loss: 0.0410 - val_loss: 0.1996\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.19780\n",
            "Epoch 103/200\n",
            "297/297 - 88s - loss: 0.0393 - val_loss: 0.1981\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.19780\n",
            "Epoch 104/200\n",
            "297/297 - 86s - loss: 0.0389 - val_loss: 0.1997\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.19780\n",
            "Epoch 105/200\n",
            "297/297 - 87s - loss: 0.0394 - val_loss: 0.1999\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.19780\n",
            "Epoch 106/200\n",
            "297/297 - 87s - loss: 0.0397 - val_loss: 0.1994\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.19780\n",
            "Epoch 107/200\n",
            "297/297 - 86s - loss: 0.0401 - val_loss: 0.2014\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.19780\n",
            "Epoch 108/200\n",
            "297/297 - 87s - loss: 0.0404 - val_loss: 0.2013\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.19780\n",
            "Epoch 109/200\n",
            "297/297 - 87s - loss: 0.0411 - val_loss: 0.2014\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.19780\n",
            "Epoch 110/200\n",
            "297/297 - 87s - loss: 0.0459 - val_loss: 0.2212\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.19780\n",
            "Epoch 111/200\n",
            "297/297 - 87s - loss: 0.0661 - val_loss: 0.2153\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.19780\n",
            "Epoch 112/200\n",
            "297/297 - 87s - loss: 0.0494 - val_loss: 0.2037\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.19780\n",
            "Epoch 113/200\n",
            "297/297 - 87s - loss: 0.0411 - val_loss: 0.2010\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.19780\n",
            "Epoch 114/200\n",
            "297/297 - 87s - loss: 0.0390 - val_loss: 0.2003\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.19780\n",
            "Epoch 115/200\n",
            "297/297 - 87s - loss: 0.0384 - val_loss: 0.2002\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.19780\n",
            "Epoch 116/200\n",
            "297/297 - 86s - loss: 0.0376 - val_loss: 0.2014\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.19780\n",
            "Epoch 117/200\n",
            "297/297 - 87s - loss: 0.0380 - val_loss: 0.2004\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.19780\n",
            "Epoch 118/200\n",
            "297/297 - 87s - loss: 0.0381 - val_loss: 0.2007\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.19780\n",
            "Epoch 119/200\n",
            "297/297 - 87s - loss: 0.0385 - val_loss: 0.2019\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.19780\n",
            "Epoch 120/200\n",
            "297/297 - 87s - loss: 0.0391 - val_loss: 0.2025\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.19780\n",
            "Epoch 121/200\n",
            "297/297 - 87s - loss: 0.0400 - val_loss: 0.2036\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.19780\n",
            "Epoch 122/200\n",
            "297/297 - 87s - loss: 0.0413 - val_loss: 0.2086\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.19780\n",
            "Epoch 123/200\n",
            "297/297 - 86s - loss: 0.0515 - val_loss: 0.2153\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.19780\n",
            "Epoch 124/200\n",
            "297/297 - 87s - loss: 0.0540 - val_loss: 0.2105\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.19780\n",
            "Epoch 125/200\n",
            "297/297 - 87s - loss: 0.0451 - val_loss: 0.2044\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.19780\n",
            "Epoch 126/200\n",
            "297/297 - 87s - loss: 0.0405 - val_loss: 0.2035\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.19780\n",
            "Epoch 127/200\n",
            "297/297 - 87s - loss: 0.0388 - val_loss: 0.2018\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.19780\n",
            "Epoch 128/200\n",
            "297/297 - 88s - loss: 0.0377 - val_loss: 0.2018\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.19780\n",
            "Epoch 129/200\n",
            "297/297 - 87s - loss: 0.0374 - val_loss: 0.2027\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.19780\n",
            "Epoch 130/200\n",
            "297/297 - 87s - loss: 0.0371 - val_loss: 0.2010\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.19780\n",
            "Epoch 131/200\n",
            "297/297 - 88s - loss: 0.0371 - val_loss: 0.2021\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.19780\n",
            "Epoch 132/200\n",
            "297/297 - 86s - loss: 0.0377 - val_loss: 0.2027\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.19780\n",
            "Epoch 133/200\n",
            "297/297 - 87s - loss: 0.0375 - val_loss: 0.2046\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.19780\n",
            "Epoch 134/200\n",
            "297/297 - 87s - loss: 0.0386 - val_loss: 0.2037\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.19780\n",
            "Epoch 135/200\n",
            "297/297 - 87s - loss: 0.0401 - val_loss: 0.2074\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.19780\n",
            "Epoch 136/200\n",
            "297/297 - 86s - loss: 0.0558 - val_loss: 0.2199\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.19780\n",
            "Epoch 137/200\n",
            "297/297 - 86s - loss: 0.0535 - val_loss: 0.2095\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.19780\n",
            "Epoch 138/200\n",
            "297/297 - 87s - loss: 0.0419 - val_loss: 0.2048\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.19780\n",
            "Epoch 139/200\n",
            "297/297 - 87s - loss: 0.0377 - val_loss: 0.2030\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.19780\n",
            "Epoch 140/200\n",
            "297/297 - 87s - loss: 0.0363 - val_loss: 0.2043\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.19780\n",
            "Epoch 141/200\n",
            "297/297 - 86s - loss: 0.0361 - val_loss: 0.2033\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.19780\n",
            "Epoch 142/200\n",
            "297/297 - 87s - loss: 0.0363 - val_loss: 0.2044\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.19780\n",
            "Epoch 143/200\n",
            "297/297 - 87s - loss: 0.0365 - val_loss: 0.2041\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.19780\n",
            "Epoch 144/200\n",
            "297/297 - 87s - loss: 0.0367 - val_loss: 0.2049\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.19780\n",
            "Epoch 145/200\n",
            "297/297 - 87s - loss: 0.0368 - val_loss: 0.2046\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.19780\n",
            "Epoch 146/200\n",
            "297/297 - 87s - loss: 0.0373 - val_loss: 0.2045\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.19780\n",
            "Epoch 147/200\n",
            "297/297 - 86s - loss: 0.0379 - val_loss: 0.2052\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.19780\n",
            "Epoch 148/200\n",
            "297/297 - 87s - loss: 0.0386 - val_loss: 0.2068\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.19780\n",
            "Epoch 149/200\n",
            "297/297 - 87s - loss: 0.0426 - val_loss: 0.2142\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.19780\n",
            "Epoch 150/200\n",
            "297/297 - 86s - loss: 0.0534 - val_loss: 0.2187\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.19780\n",
            "Epoch 151/200\n",
            "297/297 - 87s - loss: 0.0483 - val_loss: 0.2091\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.19780\n",
            "Epoch 152/200\n",
            "297/297 - 87s - loss: 0.0397 - val_loss: 0.2055\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.19780\n",
            "Epoch 153/200\n",
            "297/297 - 86s - loss: 0.0367 - val_loss: 0.2059\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.19780\n",
            "Epoch 154/200\n",
            "297/297 - 86s - loss: 0.0363 - val_loss: 0.2048\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.19780\n",
            "Epoch 155/200\n",
            "297/297 - 86s - loss: 0.0359 - val_loss: 0.2045\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.19780\n",
            "Epoch 156/200\n",
            "297/297 - 86s - loss: 0.0354 - val_loss: 0.2053\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.19780\n",
            "Epoch 157/200\n",
            "297/297 - 86s - loss: 0.0356 - val_loss: 0.2056\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.19780\n",
            "Epoch 158/200\n",
            "297/297 - 86s - loss: 0.0363 - val_loss: 0.2055\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.19780\n",
            "Epoch 159/200\n",
            "297/297 - 87s - loss: 0.0362 - val_loss: 0.2060\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.19780\n",
            "Epoch 160/200\n",
            "297/297 - 87s - loss: 0.0368 - val_loss: 0.2057\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.19780\n",
            "Epoch 161/200\n",
            "297/297 - 86s - loss: 0.0370 - val_loss: 0.2068\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.19780\n",
            "Epoch 162/200\n",
            "297/297 - 87s - loss: 0.0386 - val_loss: 0.2088\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.19780\n",
            "Epoch 163/200\n",
            "297/297 - 86s - loss: 0.0428 - val_loss: 0.2198\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.19780\n",
            "Epoch 164/200\n",
            "297/297 - 87s - loss: 0.0524 - val_loss: 0.2189\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.19780\n",
            "Epoch 165/200\n",
            "297/297 - 87s - loss: 0.0459 - val_loss: 0.2104\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.19780\n",
            "Epoch 166/200\n",
            "297/297 - 87s - loss: 0.0386 - val_loss: 0.2082\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.19780\n",
            "Epoch 167/200\n",
            "297/297 - 87s - loss: 0.0363 - val_loss: 0.2076\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.19780\n",
            "Epoch 168/200\n",
            "297/297 - 87s - loss: 0.0355 - val_loss: 0.2067\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.19780\n",
            "Epoch 169/200\n",
            "297/297 - 86s - loss: 0.0351 - val_loss: 0.2069\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.19780\n",
            "Epoch 170/200\n",
            "297/297 - 87s - loss: 0.0351 - val_loss: 0.2072\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.19780\n",
            "Epoch 171/200\n",
            "297/297 - 87s - loss: 0.0354 - val_loss: 0.2075\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.19780\n",
            "Epoch 172/200\n",
            "297/297 - 88s - loss: 0.0357 - val_loss: 0.2071\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.19780\n",
            "Epoch 173/200\n",
            "297/297 - 88s - loss: 0.0356 - val_loss: 0.2081\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.19780\n",
            "Epoch 174/200\n",
            "297/297 - 87s - loss: 0.0378 - val_loss: 0.2124\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.19780\n",
            "Epoch 175/200\n",
            "297/297 - 86s - loss: 0.0472 - val_loss: 0.2149\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.19780\n",
            "Epoch 176/200\n",
            "297/297 - 87s - loss: 0.0460 - val_loss: 0.2143\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.19780\n",
            "Epoch 177/200\n",
            "297/297 - 86s - loss: 0.0423 - val_loss: 0.2095\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.19780\n",
            "Epoch 178/200\n",
            "297/297 - 86s - loss: 0.0379 - val_loss: 0.2079\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.19780\n",
            "Epoch 179/200\n",
            "297/297 - 86s - loss: 0.0358 - val_loss: 0.2070\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.19780\n",
            "Epoch 180/200\n",
            "297/297 - 87s - loss: 0.0352 - val_loss: 0.2068\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.19780\n",
            "Epoch 181/200\n",
            "297/297 - 86s - loss: 0.0347 - val_loss: 0.2066\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.19780\n",
            "Epoch 182/200\n",
            "297/297 - 87s - loss: 0.0350 - val_loss: 0.2067\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.19780\n",
            "Epoch 183/200\n",
            "297/297 - 89s - loss: 0.0348 - val_loss: 0.2070\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.19780\n",
            "Epoch 184/200\n",
            "297/297 - 88s - loss: 0.0352 - val_loss: 0.2070\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.19780\n",
            "Epoch 185/200\n",
            "297/297 - 87s - loss: 0.0354 - val_loss: 0.2079\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.19780\n",
            "Epoch 186/200\n",
            "297/297 - 87s - loss: 0.0358 - val_loss: 0.2078\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.19780\n",
            "Epoch 187/200\n",
            "297/297 - 88s - loss: 0.0361 - val_loss: 0.2101\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.19780\n",
            "Epoch 188/200\n",
            "297/297 - 87s - loss: 0.0374 - val_loss: 0.2088\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.19780\n",
            "Epoch 189/200\n",
            "297/297 - 87s - loss: 0.0390 - val_loss: 0.2130\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.19780\n",
            "Epoch 190/200\n",
            "297/297 - 87s - loss: 0.0560 - val_loss: 0.2227\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.19780\n",
            "Epoch 191/200\n",
            "297/297 - 87s - loss: 0.0486 - val_loss: 0.2151\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.19780\n",
            "Epoch 192/200\n",
            "297/297 - 88s - loss: 0.0397 - val_loss: 0.2104\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.19780\n",
            "Epoch 193/200\n",
            "297/297 - 89s - loss: 0.0363 - val_loss: 0.2088\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.19780\n",
            "Epoch 194/200\n",
            "297/297 - 87s - loss: 0.0347 - val_loss: 0.2081\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.19780\n",
            "Epoch 195/200\n",
            "297/297 - 87s - loss: 0.0350 - val_loss: 0.2077\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.19780\n",
            "Epoch 196/200\n",
            "297/297 - 88s - loss: 0.0342 - val_loss: 0.2072\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.19780\n",
            "Epoch 197/200\n",
            "297/297 - 88s - loss: 0.0344 - val_loss: 0.2086\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.19780\n",
            "Epoch 198/200\n",
            "297/297 - 87s - loss: 0.0346 - val_loss: 0.2087\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.19780\n",
            "Epoch 199/200\n",
            "297/297 - 87s - loss: 0.0346 - val_loss: 0.2091\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.19780\n",
            "Epoch 200/200\n",
            "297/297 - 87s - loss: 0.0350 - val_loss: 0.2093\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.19780\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe7e987ecd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyBrIyGkUh6A",
        "outputId": "bd8f9de5-2565-4eaa-bf5b-445f2748a838"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-mkd-both.pkl')\n",
        "train = load_clean_sentences('english-mkd-train.pkl')\n",
        "test = load_clean_sentences('english-mkd-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:,0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare  bangla tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Macedonian Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Macedonian Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 5229\n",
            "English Max Length: 6\n",
            "Macedonian Vocabulary Size: 11684\n",
            "Macedonian Max Length: 14\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 14, 256)           2991104   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 6, 256)            0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 6, 256)            525312    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 6, 5229)           1343853   \n",
            "=================================================================\n",
            "Total params: 5,385,581\n",
            "Trainable params: 5,385,581\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "454/454 - 179s - loss: 4.3120 - val_loss: 3.7632\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.76325, saving model to model.h5\n",
            "Epoch 2/100\n",
            "454/454 - 170s - loss: 3.6343 - val_loss: 3.3962\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.76325 to 3.39620, saving model to model.h5\n",
            "Epoch 3/100\n",
            "454/454 - 169s - loss: 3.2616 - val_loss: 3.0327\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.39620 to 3.03275, saving model to model.h5\n",
            "Epoch 4/100\n",
            "454/454 - 168s - loss: 2.9201 - val_loss: 2.7015\n",
            "\n",
            "Epoch 00004: val_loss improved from 3.03275 to 2.70154, saving model to model.h5\n",
            "Epoch 5/100\n",
            "454/454 - 169s - loss: 2.5982 - val_loss: 2.3730\n",
            "\n",
            "Epoch 00005: val_loss improved from 2.70154 to 2.37297, saving model to model.h5\n",
            "Epoch 6/100\n",
            "454/454 - 168s - loss: 2.3056 - val_loss: 2.1094\n",
            "\n",
            "Epoch 00006: val_loss improved from 2.37297 to 2.10940, saving model to model.h5\n",
            "Epoch 7/100\n",
            "454/454 - 167s - loss: 2.0615 - val_loss: 1.8907\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.10940 to 1.89069, saving model to model.h5\n",
            "Epoch 8/100\n",
            "454/454 - 167s - loss: 1.8527 - val_loss: 1.6823\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.89069 to 1.68234, saving model to model.h5\n",
            "Epoch 9/100\n",
            "454/454 - 166s - loss: 1.6628 - val_loss: 1.5074\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.68234 to 1.50743, saving model to model.h5\n",
            "Epoch 10/100\n",
            "454/454 - 167s - loss: 1.4925 - val_loss: 1.3456\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.50743 to 1.34559, saving model to model.h5\n",
            "Epoch 11/100\n",
            "454/454 - 167s - loss: 1.3321 - val_loss: 1.1993\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.34559 to 1.19929, saving model to model.h5\n",
            "Epoch 12/100\n",
            "454/454 - 168s - loss: 1.1855 - val_loss: 1.0649\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.19929 to 1.06488, saving model to model.h5\n",
            "Epoch 13/100\n",
            "454/454 - 167s - loss: 1.0506 - val_loss: 0.9462\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.06488 to 0.94618, saving model to model.h5\n",
            "Epoch 14/100\n",
            "454/454 - 167s - loss: 0.9257 - val_loss: 0.8436\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.94618 to 0.84356, saving model to model.h5\n",
            "Epoch 15/100\n",
            "454/454 - 168s - loss: 0.8158 - val_loss: 0.7532\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.84356 to 0.75322, saving model to model.h5\n",
            "Epoch 16/100\n",
            "454/454 - 168s - loss: 0.7162 - val_loss: 0.6578\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.75322 to 0.65776, saving model to model.h5\n",
            "Epoch 17/100\n",
            "454/454 - 171s - loss: 0.6303 - val_loss: 0.5779\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.65776 to 0.57791, saving model to model.h5\n",
            "Epoch 18/100\n",
            "454/454 - 168s - loss: 0.5562 - val_loss: 0.5193\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.57791 to 0.51929, saving model to model.h5\n",
            "Epoch 19/100\n",
            "454/454 - 167s - loss: 0.4894 - val_loss: 0.4634\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.51929 to 0.46338, saving model to model.h5\n",
            "Epoch 20/100\n",
            "454/454 - 166s - loss: 0.4328 - val_loss: 0.4236\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.46338 to 0.42356, saving model to model.h5\n",
            "Epoch 21/100\n",
            "454/454 - 168s - loss: 0.3822 - val_loss: 0.3845\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.42356 to 0.38450, saving model to model.h5\n",
            "Epoch 22/100\n",
            "454/454 - 176s - loss: 0.3391 - val_loss: 0.3471\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.38450 to 0.34706, saving model to model.h5\n",
            "Epoch 23/100\n",
            "454/454 - 175s - loss: 0.3027 - val_loss: 0.3157\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.34706 to 0.31568, saving model to model.h5\n",
            "Epoch 24/100\n",
            "454/454 - 167s - loss: 0.2696 - val_loss: 0.2900\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.31568 to 0.28995, saving model to model.h5\n",
            "Epoch 25/100\n",
            "454/454 - 176s - loss: 0.2415 - val_loss: 0.2693\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.28995 to 0.26925, saving model to model.h5\n",
            "Epoch 26/100\n",
            "454/454 - 166s - loss: 0.2156 - val_loss: 0.2499\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.26925 to 0.24986, saving model to model.h5\n",
            "Epoch 27/100\n",
            "454/454 - 167s - loss: 0.1947 - val_loss: 0.2353\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.24986 to 0.23533, saving model to model.h5\n",
            "Epoch 28/100\n",
            "454/454 - 176s - loss: 0.1746 - val_loss: 0.2217\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.23533 to 0.22166, saving model to model.h5\n",
            "Epoch 29/100\n",
            "454/454 - 175s - loss: 0.1614 - val_loss: 0.2081\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.22166 to 0.20813, saving model to model.h5\n",
            "Epoch 30/100\n",
            "454/454 - 165s - loss: 0.1492 - val_loss: 0.2010\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.20813 to 0.20105, saving model to model.h5\n",
            "Epoch 31/100\n",
            "454/454 - 166s - loss: 0.1383 - val_loss: 0.1900\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.20105 to 0.19002, saving model to model.h5\n",
            "Epoch 32/100\n",
            "454/454 - 166s - loss: 0.1251 - val_loss: 0.1881\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.19002 to 0.18814, saving model to model.h5\n",
            "Epoch 33/100\n",
            "454/454 - 166s - loss: 0.1192 - val_loss: 0.1799\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.18814 to 0.17994, saving model to model.h5\n",
            "Epoch 34/100\n",
            "454/454 - 167s - loss: 0.1128 - val_loss: 0.1766\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.17994 to 0.17659, saving model to model.h5\n",
            "Epoch 35/100\n",
            "454/454 - 167s - loss: 0.1065 - val_loss: 0.1703\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.17659 to 0.17034, saving model to model.h5\n",
            "Epoch 36/100\n",
            "454/454 - 167s - loss: 0.1018 - val_loss: 0.1680\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.17034 to 0.16799, saving model to model.h5\n",
            "Epoch 37/100\n",
            "454/454 - 167s - loss: 0.0971 - val_loss: 0.1639\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.16799 to 0.16395, saving model to model.h5\n",
            "Epoch 38/100\n",
            "454/454 - 168s - loss: 0.0946 - val_loss: 0.1615\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.16395 to 0.16147, saving model to model.h5\n",
            "Epoch 39/100\n",
            "454/454 - 171s - loss: 0.0886 - val_loss: 0.1584\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.16147 to 0.15836, saving model to model.h5\n",
            "Epoch 40/100\n",
            "454/454 - 168s - loss: 0.0853 - val_loss: 0.1597\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.15836\n",
            "Epoch 41/100\n",
            "454/454 - 168s - loss: 0.0857 - val_loss: 0.1543\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.15836 to 0.15430, saving model to model.h5\n",
            "Epoch 42/100\n",
            "454/454 - 177s - loss: 0.0828 - val_loss: 0.1563\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.15430\n",
            "Epoch 43/100\n",
            "454/454 - 177s - loss: 0.0803 - val_loss: 0.1543\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.15430 to 0.15429, saving model to model.h5\n",
            "Epoch 44/100\n",
            "454/454 - 179s - loss: 0.0788 - val_loss: 0.1530\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.15429 to 0.15304, saving model to model.h5\n",
            "Epoch 45/100\n",
            "454/454 - 172s - loss: 0.0764 - val_loss: 0.1507\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.15304 to 0.15066, saving model to model.h5\n",
            "Epoch 46/100\n",
            "454/454 - 166s - loss: 0.0758 - val_loss: 0.1526\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.15066\n",
            "Epoch 47/100\n",
            "454/454 - 167s - loss: 0.0739 - val_loss: 0.1527\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.15066\n",
            "Epoch 48/100\n",
            "454/454 - 167s - loss: 0.0715 - val_loss: 0.1481\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.15066 to 0.14813, saving model to model.h5\n",
            "Epoch 49/100\n",
            "454/454 - 168s - loss: 0.0714 - val_loss: 0.1503\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.14813\n",
            "Epoch 50/100\n",
            "454/454 - 167s - loss: 0.0706 - val_loss: 0.1507\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.14813\n",
            "Epoch 51/100\n",
            "454/454 - 167s - loss: 0.0702 - val_loss: 0.1513\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.14813\n",
            "Epoch 52/100\n",
            "454/454 - 167s - loss: 0.0704 - val_loss: 0.1488\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.14813\n",
            "Epoch 53/100\n",
            "454/454 - 169s - loss: 0.0691 - val_loss: 0.1502\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.14813\n",
            "Epoch 54/100\n",
            "454/454 - 167s - loss: 0.0697 - val_loss: 0.1484\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.14813\n",
            "Epoch 55/100\n",
            "454/454 - 167s - loss: 0.0667 - val_loss: 0.1465\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.14813 to 0.14648, saving model to model.h5\n",
            "Epoch 56/100\n",
            "454/454 - 167s - loss: 0.0650 - val_loss: 0.1465\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.14648 to 0.14647, saving model to model.h5\n",
            "Epoch 57/100\n",
            "454/454 - 167s - loss: 0.0656 - val_loss: 0.1463\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.14647 to 0.14628, saving model to model.h5\n",
            "Epoch 58/100\n",
            "454/454 - 168s - loss: 0.0647 - val_loss: 0.1484\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.14628\n",
            "Epoch 59/100\n",
            "454/454 - 168s - loss: 0.0658 - val_loss: 0.1485\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.14628\n",
            "Epoch 60/100\n",
            "454/454 - 167s - loss: 0.0671 - val_loss: 0.1491\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.14628\n",
            "Epoch 61/100\n",
            "454/454 - 167s - loss: 0.0649 - val_loss: 0.1456\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.14628 to 0.14561, saving model to model.h5\n",
            "Epoch 62/100\n",
            "454/454 - 167s - loss: 0.0639 - val_loss: 0.1464\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.14561\n",
            "Epoch 63/100\n",
            "454/454 - 168s - loss: 0.0628 - val_loss: 0.1455\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.14561 to 0.14547, saving model to model.h5\n",
            "Epoch 64/100\n",
            "454/454 - 167s - loss: 0.0616 - val_loss: 0.1441\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.14547 to 0.14413, saving model to model.h5\n",
            "Epoch 65/100\n",
            "454/454 - 167s - loss: 0.0608 - val_loss: 0.1457\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.14413\n",
            "Epoch 66/100\n",
            "454/454 - 167s - loss: 0.0607 - val_loss: 0.1455\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.14413\n",
            "Epoch 67/100\n",
            "454/454 - 167s - loss: 0.0614 - val_loss: 0.1446\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.14413\n",
            "Epoch 68/100\n",
            "454/454 - 168s - loss: 0.0592 - val_loss: 0.1469\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.14413\n",
            "Epoch 69/100\n",
            "454/454 - 169s - loss: 0.0612 - val_loss: 0.1451\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.14413\n",
            "Epoch 70/100\n",
            "454/454 - 169s - loss: 0.0606 - val_loss: 0.1483\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.14413\n",
            "Epoch 71/100\n",
            "454/454 - 178s - loss: 0.0619 - val_loss: 0.1493\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.14413\n",
            "Epoch 72/100\n",
            "454/454 - 171s - loss: 0.0615 - val_loss: 0.1460\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.14413\n",
            "Epoch 73/100\n",
            "454/454 - 169s - loss: 0.0592 - val_loss: 0.1439\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.14413 to 0.14387, saving model to model.h5\n",
            "Epoch 74/100\n",
            "454/454 - 167s - loss: 0.0574 - val_loss: 0.1454\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.14387\n",
            "Epoch 75/100\n",
            "454/454 - 168s - loss: 0.0577 - val_loss: 0.1457\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.14387\n",
            "Epoch 76/100\n",
            "454/454 - 168s - loss: 0.0569 - val_loss: 0.1443\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.14387\n",
            "Epoch 77/100\n",
            "454/454 - 167s - loss: 0.0565 - val_loss: 0.1435\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.14387 to 0.14346, saving model to model.h5\n",
            "Epoch 78/100\n",
            "454/454 - 167s - loss: 0.0575 - val_loss: 0.1473\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.14346\n",
            "Epoch 79/100\n",
            "454/454 - 168s - loss: 0.0582 - val_loss: 0.1454\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.14346\n",
            "Epoch 80/100\n",
            "454/454 - 167s - loss: 0.0590 - val_loss: 0.1468\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.14346\n",
            "Epoch 81/100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNjApbfW-_3u",
        "outputId": "810c3b0b-7e2f-4f36-999c-87d515ddd752"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-mkd-both.pkl')\n",
        "train = load_clean_sentences('english-mkd-train.pkl')\n",
        "test = load_clean_sentences('english-mkd-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[Том живее во близина], target=[Tom lives close by], predicted=[tom lives close by]\n",
            "src=[Тука ми е поарно], target=[Im better off here], predicted=[im better off here]\n",
            "src=[Побогат сум од тебе], target=[Im richer than you], predicted=[im richer than you]\n",
            "src=[Том ја прати Мери да си оди], target=[Tom sent Mary away], predicted=[tom sent mary away]\n",
            "src=[Том е влијателен], target=[Tom is influential], predicted=[tom is influential]\n",
            "src=[Следи го], target=[Follow him], predicted=[follow him]\n",
            "src=[Ќе се вратам во шест и пол], target=[Ill return at 630], predicted=[ill return at 630]\n",
            "src=[Колку си беден], target=[Youre so pathetic], predicted=[youre so pathetic]\n",
            "src=[Дома сум], target=[Im at home], predicted=[im home]\n",
            "src=[Спушти го тој пиштол], target=[Put down that gun], predicted=[put down that gun]\n",
            "BLEU-1: 0.674925\n",
            "BLEU-2: 0.605221\n",
            "BLEU-3: 0.537066\n",
            "BLEU-4: 0.323117\n",
            "test\n",
            "src=[Том збесна], target=[Tom got furious], predicted=[tom became furious]\n",
            "src=[Можеме да се обидеме], target=[We can try], predicted=[we can try]\n",
            "src=[Ќе биде ли Том спремен], target=[Will Tom be ready], predicted=[will tom be ready]\n",
            "src=[Треба да тргам], target=[I should get going], predicted=[i should get going]\n",
            "src=[Слави ли Том], target=[Is Tom celebrating], predicted=[is tom celebrating]\n",
            "src=[Том сите ги нервира], target=[Tom bugs everyone], predicted=[tom bugs everyone]\n",
            "src=[Влегувај внатре], target=[Get inside], predicted=[get inside]\n",
            "src=[Слушнавме пукот], target=[We heard a gunshot], predicted=[we heard a gunshot]\n",
            "src=[Донеси му вода на Том], target=[Get Tom some water], predicted=[get tom some water]\n",
            "src=[Тој ми е стар пријател], target=[Hes my old friend], predicted=[he my my old]\n",
            "BLEU-1: 0.653591\n",
            "BLEU-2: 0.582543\n",
            "BLEU-3: 0.516964\n",
            "BLEU-4: 0.308605\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ik1hzaKGfEJ",
        "outputId": "8cc36b4d-7032-412a-afd6-2239c0e220ad"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 100:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-mkd-both.pkl')\n",
        "train = load_clean_sentences('english-mkd-train.pkl')\n",
        "test = load_clean_sentences('english-mkd-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[Том живее во близина], target=[Tom lives close by], predicted=[tom lives close by]\n",
            "src=[Тука ми е поарно], target=[Im better off here], predicted=[im better off here]\n",
            "src=[Побогат сум од тебе], target=[Im richer than you], predicted=[im richer than you]\n",
            "src=[Том ја прати Мери да си оди], target=[Tom sent Mary away], predicted=[tom sent mary away]\n",
            "src=[Том е влијателен], target=[Tom is influential], predicted=[tom is influential]\n",
            "src=[Следи го], target=[Follow him], predicted=[follow him]\n",
            "src=[Ќе се вратам во шест и пол], target=[Ill return at 630], predicted=[ill return at 630]\n",
            "src=[Колку си беден], target=[Youre so pathetic], predicted=[youre so pathetic]\n",
            "src=[Дома сум], target=[Im at home], predicted=[im home]\n",
            "src=[Спушти го тој пиштол], target=[Put down that gun], predicted=[put down that gun]\n",
            "src=[Ни треба физичка активност], target=[We need exercise], predicted=[we need exercise]\n",
            "src=[Прилично сме паметни], target=[Were pretty smart], predicted=[were pretty smart]\n",
            "src=[Ова е срамота], target=[This is shameful], predicted=[this is shameful]\n",
            "src=[Ме виделе ли како си одам], target=[Was I seen leaving], predicted=[was i seen leaving]\n",
            "src=[Се здобив со тоа], target=[I acquired it], predicted=[i acquired it]\n",
            "src=[Нема многу да издржи], target=[It wont last long], predicted=[it wont last long]\n",
            "src=[Сакаш ли да црташ], target=[Do you like to draw], predicted=[do you like to draw]\n",
            "src=[Веќе ми недостига Том], target=[I already miss Tom], predicted=[i already miss tom]\n",
            "src=[Какви ти се оценките], target=[How are your grades], predicted=[how are your grades]\n",
            "src=[Се изгубивме], target=[Were lost], predicted=[were lost]\n",
            "src=[Вознемирен сум во врска со Том], target=[Im upset about Tom], predicted=[im upset about tom]\n",
            "src=[Таа е почетничка], target=[She is a beginner], predicted=[she is a beginner]\n",
            "src=[Том те памти], target=[Tom remembers you], predicted=[tom remembers you]\n",
            "src=[Ножот отапел], target=[The knife is dull], predicted=[the knife is dull]\n",
            "src=[Мислам дека сум згрешил], target=[I think I was wrong], predicted=[i think i was wrong]\n",
            "src=[Торбата е полна], target=[The bag is full], predicted=[the bag is full]\n",
            "src=[Само ова ли ни треба], target=[Is this all we need], predicted=[is this all we need]\n",
            "src=[Батали], target=[Never mind], predicted=[never mind]\n",
            "src=[Не се повлекувам], target=[Im not backing out], predicted=[im not backing out]\n",
            "src=[Кога го користиш], target=[When do you use it], predicted=[when do you use it]\n",
            "src=[Сам сум тука], target=[Im here on my own], predicted=[im here on my own]\n",
            "src=[Чекам], target=[Im waiting], predicted=[im waiting]\n",
            "src=[Том почна да се смее], target=[Tom began to laugh], predicted=[tom began laughing]\n",
            "src=[Многу помогна], target=[It was very helpful], predicted=[it was very helpful]\n",
            "src=[Том е јак], target=[Tom rocks], predicted=[tom rocks cool]\n",
            "src=[Напиј се лек], target=[Take your medicine], predicted=[take your medicine]\n",
            "src=[Се здебелуваме], target=[Were getting fat], predicted=[were getting fat]\n",
            "src=[Том ја клоцна кантата], target=[Tom kicked the can], predicted=[tom kicked the can]\n",
            "src=[Време ти е за спиење], target=[Its your bedtime], predicted=[its your bedtime]\n",
            "src=[Том е на врата], target=[Tom is at the door], predicted=[tom is at the door]\n",
            "src=[Соработуваме], target=[Were cooperating], predicted=[were cooperating]\n",
            "src=[Да ти верувам Том], target=[I do trust you Tom], predicted=[i do trust you tom]\n",
            "src=[Не биди таков циција], target=[Dont be so stingy], predicted=[dont be so stingy]\n",
            "src=[Том го застрелале], target=[Tom has been shot], predicted=[tom has been shot]\n",
            "src=[Штотуку се венчав], target=[I just got married], predicted=[i just got married]\n",
            "src=[Пак лажеш], target=[Youre lying again], predicted=[youre lying again]\n",
            "src=[Морав пак да лажам], target=[I had to lie again], predicted=[i had to lie again]\n",
            "src=[Том сака сирење], target=[Tom likes cheese], predicted=[tom likes cheese]\n",
            "src=[Влезе во мојата соба], target=[He entered my room], predicted=[he entered my room]\n",
            "src=[Имам намера така да направам], target=[I intend to do that], predicted=[i intend to do that]\n",
            "src=[Не е прерано], target=[Its not too early], predicted=[its not too early]\n",
            "src=[Одам накај југ], target=[Im going south], predicted=[im going south]\n",
            "src=[Сите слушаа], target=[They all listened], predicted=[they all listened]\n",
            "src=[Што е ова чудо], target=[Whats this thing], predicted=[whats this this]\n",
            "src=[Ајде де], target=[Go on ahead], predicted=[go on ahead]\n",
            "src=[Сакавме да чекаме], target=[We wanted to wait], predicted=[we wanted to wait]\n",
            "src=[Ќе се покаеш за ова], target=[Youll regret this], predicted=[youll regret this]\n",
            "src=[Том ни готви], target=[Tom cooks for us], predicted=[tom cooks for us]\n",
            "src=[Том се згрози], target=[Tom was disgusted], predicted=[tom was disgusted]\n",
            "src=[Зарем не е чудно тоа], target=[Isnt that strange], predicted=[isnt that strange]\n",
            "src=[Не поднесувам лажливци], target=[I cant stand liars], predicted=[i cant stand liars]\n",
            "src=[Зошто се крие Том], target=[Why is Tom hiding], predicted=[why is tom hiding]\n",
            "src=[Не бев убеден], target=[I wasnt convinced], predicted=[i wasnt convinced]\n",
            "src=[Не ми оди тенис], target=[Im bad at tennis], predicted=[im bad at tennis]\n",
            "src=[Том е експерт], target=[Toms an expert], predicted=[toms an expert]\n",
            "src=[Пробај да седиш мирен], target=[Try to stand still], predicted=[try to stand still]\n",
            "src=[Том е несопирлив], target=[Tom is unstoppable], predicted=[toms unstoppable]\n",
            "src=[Користи акрилни бои], target=[Use acrylic paint], predicted=[use acrylic paint]\n",
            "src=[Ќе спијам], target=[Im going to sleep], predicted=[im going to sleep]\n",
            "src=[Мене тоа не би ми пречело], target=[Id be OK with that], predicted=[id be ok with that]\n",
            "src=[Ќе бидам тука], target=[Ill be right here], predicted=[ill be right here]\n",
            "src=[Том беше заинтересиран], target=[Tom was interested], predicted=[tom was intrigued]\n",
            "src=[Ми студи и гладен сум], target=[Im cold and hungry], predicted=[im cold and hungry]\n",
            "src=[Се расчистив со него], target=[I got even with him], predicted=[i got even with him]\n",
            "src=[Том е доста мрзлив], target=[Tom is quite lazy], predicted=[tom is quite lazy]\n",
            "src=[Том е чуден], target=[Tom is odd], predicted=[tom weird]\n",
            "src=[Бирово е мое], target=[This desk is mine], predicted=[this desk is mine]\n",
            "src=[Ние сме месари], target=[Were butchers], predicted=[were butchers]\n",
            "src=[Нели е тоа необично], target=[Isnt that curious], predicted=[isnt that curious]\n",
            "src=[Не си лузер], target=[Youre not a loser], predicted=[youre not a loser]\n",
            "src=[Тоа е ѓубре], target=[Its garbage], predicted=[its garbage]\n",
            "src=[Том не ми кажа], target=[Tom didnt tell me], predicted=[tom didnt tell me]\n",
            "src=[Ме боли колкот], target=[My hip hurts], predicted=[my hip hurts]\n",
            "src=[Само се шалев], target=[I was just kidding], predicted=[i was just kidding]\n",
            "src=[Не бев сам тогаш], target=[I wasnt alone then], predicted=[i wasnt alone then]\n",
            "src=[Сите го знаат тоа], target=[Everyone knows that], predicted=[everyone knows that]\n",
            "src=[Не шпекулирам], target=[Im not speculating], predicted=[im not speculating]\n",
            "src=[Мразам да ризикувам], target=[I hate taking risks], predicted=[i hate taking risks]\n",
            "src=[Том изгледа осамено], target=[Tom seems lonely], predicted=[tom seems lonely]\n",
            "src=[Том сака да работи], target=[Tom wants to work], predicted=[tom wants to work]\n",
            "src=[Дај да се испијаниме], target=[Lets get drunk], predicted=[lets get drunk]\n",
            "src=[Можно е да успее], target=[It just might work], predicted=[it just might work]\n",
            "src=[Тоа ли е сѐ], target=[Is that all], predicted=[is that all]\n",
            "src=[Го прашав Том], target=[I asked Tom], predicted=[i asked tom]\n",
            "src=[Ја имаш нашата понуда], target=[You have our offer], predicted=[you have our offer]\n",
            "src=[Тој си ја врши должноста], target=[He does his duty], predicted=[he does his duty]\n",
            "src=[Возот застана], target=[The train stopped], predicted=[the train stopped]\n",
            "src=[Се е бело], target=[Everything is white], predicted=[everything is white]\n",
            "src=[Не можам да ги кривам], target=[I cant blame them], predicted=[i cant blame them]\n",
            "src=[Не се сомневам дека си во право], target=[I bet youre right], predicted=[i bet youre right]\n",
            "BLEU-1: 0.674925\n",
            "BLEU-2: 0.605221\n",
            "BLEU-3: 0.537066\n",
            "BLEU-4: 0.323117\n",
            "test\n",
            "src=[Том збесна], target=[Tom got furious], predicted=[tom became furious]\n",
            "src=[Можеме да се обидеме], target=[We can try], predicted=[we can try]\n",
            "src=[Ќе биде ли Том спремен], target=[Will Tom be ready], predicted=[will tom be ready]\n",
            "src=[Треба да тргам], target=[I should get going], predicted=[i should get going]\n",
            "src=[Слави ли Том], target=[Is Tom celebrating], predicted=[is tom celebrating]\n",
            "src=[Том сите ги нервира], target=[Tom bugs everyone], predicted=[tom bugs everyone]\n",
            "src=[Влегувај внатре], target=[Get inside], predicted=[get inside]\n",
            "src=[Слушнавме пукот], target=[We heard a gunshot], predicted=[we heard a gunshot]\n",
            "src=[Донеси му вода на Том], target=[Get Tom some water], predicted=[get tom some water]\n",
            "src=[Тој ми е стар пријател], target=[Hes my old friend], predicted=[he my my old]\n",
            "src=[Прилично сум начукан], target=[Im pretty zonked], predicted=[im pretty zonked]\n",
            "src=[Том е нерешителен], target=[Tom is indecisive], predicted=[tom is indecisive]\n",
            "src=[Има ли некој таму], target=[Is someone in there], predicted=[is somebody in]\n",
            "src=[И двајцата те мразиме], target=[We both hate you], predicted=[we both hate you]\n",
            "src=[Том има мечта], target=[Toms imaginative], predicted=[toms imaginative]\n",
            "src=[Фин си], target=[Youre nice], predicted=[youre nice]\n",
            "src=[Насмеј се], target=[Smile], predicted=[smile]\n",
            "src=[Том се однесува откачено], target=[Tom is acting nuts], predicted=[tom is acting nuts]\n",
            "src=[Како беше концертот], target=[How was the concert], predicted=[how was the concert]\n",
            "src=[Том влезе во таксито], target=[Tom got in the cab], predicted=[tom got in the cab]\n",
            "src=[Тргај ми се од патот], target=[Keep out of my way], predicted=[keep out of my way]\n",
            "src=[Том простенка], target=[Tom groaned], predicted=[tom groaned]\n",
            "src=[Ќе погледаме], target=[Well check it out], predicted=[well check it out]\n",
            "src=[Алармот одѕвони], target=[The alarm sounded], predicted=[the alarm sounded]\n",
            "src=[Том звучеше вознемирено], target=[Tom sounded upset], predicted=[tom sounded upset]\n",
            "src=[Прва година сум], target=[Im just a freshman], predicted=[im just a freshman]\n",
            "src=[Угаси го огнот], target=[Put that fire out], predicted=[put that fire out]\n",
            "src=[Пушти ме да продолжам], target=[Let me continue], predicted=[let me continue]\n",
            "src=[Том е среќен], target=[Toms glad], predicted=[toms glad]\n",
            "src=[Не можеш да победиш Том], target=[You cant win Tom], predicted=[you cant win tom]\n",
            "src=[Отворени ли ти се очите], target=[Are your eyes open], predicted=[are your eyes open]\n",
            "src=[Отворено е], target=[Its open], predicted=[its open]\n",
            "src=[Зарем не е прекрасно], target=[Isnt it gorgeous], predicted=[isnt it gorgeous]\n",
            "src=[Само ќе си седам тука], target=[Ill just sit here], predicted=[ill just sit here]\n",
            "src=[Само го кренав], target=[I just picked it up], predicted=[i just picked it up]\n",
            "src=[Нормално], target=[Of course], predicted=[of course]\n",
            "src=[Буден сум], target=[Im awake], predicted=[im awake]\n",
            "src=[Нема да постапат така], target=[They wont do that], predicted=[they wont do that]\n",
            "src=[Том е преоптеретен], target=[Tom is overwhelmed], predicted=[toms is]\n",
            "src=[Не сум неписмен], target=[Im not illiterate], predicted=[im not illiterate]\n",
            "src=[Сите го направија тоа], target=[Everyone did it], predicted=[everyone did it]\n",
            "src=[Ќе наминам попосле], target=[Ill stop by later], predicted=[ill stop by later]\n",
            "src=[Штотуку видов дух], target=[I just saw a ghost], predicted=[i just saw a ghost]\n",
            "src=[Никого не гледам], target=[I dont see anyone], predicted=[i dont see anyone]\n",
            "src=[Том изгледа сомнително], target=[Tom looks dubious], predicted=[tom looks dubious]\n",
            "src=[Том изгледа неспокојно], target=[Tom looks restless], predicted=[tom looks restless]\n",
            "src=[Том е секогаш љубезен], target=[Tom is always nice], predicted=[tom is always nice]\n",
            "src=[Том е специфичен], target=[Tom is particular], predicted=[tom is particular]\n",
            "src=[Не бев организиран], target=[I wasnt organized], predicted=[i wasnt organized]\n",
            "src=[Том изгледа немирно], target=[Tom looks restless], predicted=[tom looks restless]\n",
            "src=[Станав околу пет], target=[I got up about five], predicted=[i got up about five]\n",
            "src=[Нема сомнеж], target=[There is no doubt], predicted=[its a sure thing]\n",
            "src=[Зошто се плаши Том], target=[Why is Tom scared], predicted=[why is tom scared]\n",
            "src=[Том многу сака лалиња], target=[Tom loves tulips], predicted=[tom loves tulips]\n",
            "src=[Одам јас во теретана], target=[Im off to the gym], predicted=[im off to the gym]\n",
            "src=[Том е возлен], target=[Toms furious], predicted=[toms furious]\n",
            "src=[Леле во право си], target=[Wow youre right], predicted=[wow youre right]\n",
            "src=[Том има три коли], target=[Tom has three cars], predicted=[tom has three cars]\n",
            "src=[Мило ми е што се сложуваш], target=[Im glad you agree], predicted=[im glad you agree]\n",
            "src=[Дај ми чепкалка], target=[Give me a toothpick], predicted=[give me a toothpick]\n",
            "src=[Инвестиција е], target=[Its an investment], predicted=[its an investment]\n",
            "src=[Смее ли да останам], target=[Can I stay], predicted=[can i stay]\n",
            "src=[Го оставам Том да спие до подоцна], target=[I let Tom sleep in], predicted=[i let tom sleep in]\n",
            "src=[Ете го докторот], target=[Theres the doctor], predicted=[theres the doctor]\n",
            "src=[Таа се прослави], target=[She became famous], predicted=[she became famous]\n",
            "src=[Едвај стојам на нозе], target=[I can hardly stand], predicted=[i can hardly stand]\n",
            "src=[Немаш одбрана], target=[Youre defenseless], predicted=[youre defenseless]\n",
            "src=[Само одбери еден], target=[Just pick one out], predicted=[just pick one out]\n",
            "src=[Немаме одбрана], target=[Were defenseless], predicted=[were defenseless]\n",
            "src=[Не се согласувам со тебе], target=[I disagree with you], predicted=[i disagree with you]\n",
            "src=[Ти ли си тоа], target=[Is it you], predicted=[is it you]\n",
            "src=[Кој престана], target=[Who stopped], predicted=[who stopped]\n",
            "src=[Кој ти помогнал], target=[Who helped you], predicted=[who helped you]\n",
            "src=[Те спасив], target=[I saved you], predicted=[i saved you]\n",
            "src=[Сакам прошетки], target=[I like taking walks], predicted=[i like taking walks]\n",
            "src=[Пардон], target=[Excuse me], predicted=[excuse me]\n",
            "src=[Заклучи си ја вратата], target=[Lock your door], predicted=[lock your door]\n",
            "src=[Тој обожава да озборува], target=[He loves to gossip], predicted=[he loves to gossip]\n",
            "src=[Кој те изненади], target=[Who surprised you], predicted=[who surprised you]\n",
            "src=[Ништо не им кажав], target=[I told them nothing], predicted=[i told them nothing]\n",
            "src=[Том не се појави], target=[Tom didnt turn up], predicted=[tom didnt turn up]\n",
            "src=[Имав ли избор], target=[Did I have a choice], predicted=[did i have a choice]\n",
            "src=[Татко ми има работа], target=[My father is busy], predicted=[my father is busy]\n",
            "src=[Не се малоумни], target=[Theyre not stupid], predicted=[theyre not stupid]\n",
            "src=[Том ми е партнер], target=[Tom is my partner], predicted=[tom is my partner]\n",
            "src=[Том е сѐ уште зашеметен], target=[Tom is still woozy], predicted=[tom is still woozy]\n",
            "src=[Том го извади капакот], target=[Tom opened the lid], predicted=[tom opened the lid]\n",
            "src=[Сега сме пријатели], target=[Were friends now], predicted=[were friends now]\n",
            "src=[Може ли и за мене], target=[Can I have one too], predicted=[can i have one too]\n",
            "src=[Том е црнокос], target=[Tom has black hair], predicted=[tom has black hair]\n",
            "src=[Се спремав], target=[Ive been preparing], predicted=[ive been preparing]\n",
            "src=[Не сакам да трчам], target=[I dont like to run], predicted=[i dont like to run]\n",
            "src=[Том си ги врза чевлите], target=[Tom tied his shoes], predicted=[tom tied his shoes]\n",
            "src=[Прифаќам подароци], target=[I accept gifts], predicted=[i accept gifts]\n",
            "src=[Тоа е навистина добро], target=[Thats really good], predicted=[thats really good]\n",
            "src=[Добра приказна е], target=[Its a good story], predicted=[its a good story]\n",
            "src=[Ќе нарачам пица], target=[Im ordering pizza], predicted=[im ordering pizza]\n",
            "src=[Том штотуку предложи брак], target=[Tom just proposed], predicted=[tom just proposed]\n",
            "src=[Ни требаат парите], target=[We need the money], predicted=[we need the money]\n",
            "src=[Том е многу суетен], target=[Tom has a huge ego], predicted=[tom has a huge ego]\n",
            "BLEU-1: 0.653591\n",
            "BLEU-2: 0.582543\n",
            "BLEU-3: 0.516964\n",
            "BLEU-4: 0.308605\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxbfz0oJ3QoQ",
        "outputId": "0f435185-312f-453c-fad9-98a595d71335"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 100:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-mkd-both.pkl')\n",
        "train = load_clean_sentences('english-mkd-train.pkl')\n",
        "test = load_clean_sentences('english-mkd-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[Том живее во близина], target=[Tom lives close by], predicted=[tom lives close by]\n",
            "src=[Тука ми е поарно], target=[Im better off here], predicted=[im better off here]\n",
            "src=[Побогат сум од тебе], target=[Im richer than you], predicted=[im richer than you]\n",
            "src=[Том ја прати Мери да си оди], target=[Tom sent Mary away], predicted=[tom sent mary away]\n",
            "src=[Том е влијателен], target=[Tom is influential], predicted=[tom is influential]\n",
            "src=[Следи го], target=[Follow him], predicted=[follow him]\n",
            "src=[Ќе се вратам во шест и пол], target=[Ill return at 630], predicted=[ill return at 630]\n",
            "src=[Колку си беден], target=[Youre so pathetic], predicted=[youre so pathetic]\n",
            "src=[Дома сум], target=[Im at home], predicted=[im home home]\n",
            "src=[Спушти го тој пиштол], target=[Put down that gun], predicted=[put down that gun]\n",
            "src=[Ни треба физичка активност], target=[We need exercise], predicted=[we need exercise]\n",
            "src=[Прилично сме паметни], target=[Were pretty smart], predicted=[were pretty smart]\n",
            "src=[Ова е срамота], target=[This is shameful], predicted=[this is shameful]\n",
            "src=[Ме виделе ли како си одам], target=[Was I seen leaving], predicted=[was i seen leaving]\n",
            "src=[Се здобив со тоа], target=[I acquired it], predicted=[i acquired it]\n",
            "src=[Нема многу да издржи], target=[It wont last long], predicted=[it wont last long]\n",
            "src=[Сакаш ли да црташ], target=[Do you like to draw], predicted=[do you like to draw]\n",
            "src=[Веќе ми недостига Том], target=[I already miss Tom], predicted=[i already miss tom]\n",
            "src=[Какви ти се оценките], target=[How are your grades], predicted=[how are your grades]\n",
            "src=[Се изгубивме], target=[Were lost], predicted=[were lost]\n",
            "src=[Вознемирен сум во врска со Том], target=[Im upset about Tom], predicted=[im upset about tom]\n",
            "src=[Таа е почетничка], target=[She is a beginner], predicted=[she is a beginner]\n",
            "src=[Том те памти], target=[Tom remembers you], predicted=[tom remembers you]\n",
            "src=[Ножот отапел], target=[The knife is dull], predicted=[the knife is dull]\n",
            "src=[Мислам дека сум згрешил], target=[I think I was wrong], predicted=[i think i was wrong]\n",
            "src=[Торбата е полна], target=[The bag is full], predicted=[the bag is full]\n",
            "src=[Само ова ли ни треба], target=[Is this all we need], predicted=[is this all we need]\n",
            "src=[Батали], target=[Never mind], predicted=[never mind]\n",
            "src=[Не се повлекувам], target=[Im not backing out], predicted=[im not backing out]\n",
            "src=[Кога го користиш], target=[When do you use it], predicted=[when do you use it]\n",
            "src=[Сам сум тука], target=[Im here on my own], predicted=[im here on my own]\n",
            "src=[Чекам], target=[Im waiting], predicted=[im waiting]\n",
            "src=[Том почна да се смее], target=[Tom began to laugh], predicted=[tom began to]\n",
            "src=[Многу помогна], target=[It was very helpful], predicted=[it was very helpful]\n",
            "src=[Том е јак], target=[Tom rocks], predicted=[tom is cool]\n",
            "src=[Напиј се лек], target=[Take your medicine], predicted=[take your medicine]\n",
            "src=[Се здебелуваме], target=[Were getting fat], predicted=[were getting fat]\n",
            "src=[Том ја клоцна кантата], target=[Tom kicked the can], predicted=[tom kicked the can]\n",
            "src=[Време ти е за спиење], target=[Its your bedtime], predicted=[its your bedtime]\n",
            "src=[Том е на врата], target=[Tom is at the door], predicted=[tom is at the door]\n",
            "src=[Соработуваме], target=[Were cooperating], predicted=[were cooperating]\n",
            "src=[Да ти верувам Том], target=[I do trust you Tom], predicted=[i do trust you tom]\n",
            "src=[Не биди таков циција], target=[Dont be so stingy], predicted=[dont be so stingy]\n",
            "src=[Том го застрелале], target=[Tom has been shot], predicted=[tom has been shot]\n",
            "src=[Штотуку се венчав], target=[I just got married], predicted=[i just got married]\n",
            "src=[Пак лажеш], target=[Youre lying again], predicted=[youre lying again]\n",
            "src=[Морав пак да лажам], target=[I had to lie again], predicted=[i had to lie again]\n",
            "src=[Том сака сирење], target=[Tom likes cheese], predicted=[tom likes cheese]\n",
            "src=[Влезе во мојата соба], target=[He entered my room], predicted=[he entered my room]\n",
            "src=[Имам намера така да направам], target=[I intend to do that], predicted=[i intend to do that]\n",
            "src=[Не е прерано], target=[Its not too early], predicted=[its not too early]\n",
            "src=[Одам накај југ], target=[Im going south], predicted=[im going south]\n",
            "src=[Сите слушаа], target=[They all listened], predicted=[they all listened]\n",
            "src=[Што е ова чудо], target=[Whats this thing], predicted=[whats this thing thing]\n",
            "src=[Ајде де], target=[Go on ahead], predicted=[go on ahead]\n",
            "src=[Сакавме да чекаме], target=[We wanted to wait], predicted=[we wanted to wait]\n",
            "src=[Ќе се покаеш за ова], target=[Youll regret this], predicted=[youll regret this]\n",
            "src=[Том ни готви], target=[Tom cooks for us], predicted=[tom cooks for us]\n",
            "src=[Том се згрози], target=[Tom was disgusted], predicted=[tom was disgusted]\n",
            "src=[Зарем не е чудно тоа], target=[Isnt that strange], predicted=[isnt that strange]\n",
            "src=[Не поднесувам лажливци], target=[I cant stand liars], predicted=[i cant stand liars]\n",
            "src=[Зошто се крие Том], target=[Why is Tom hiding], predicted=[why is tom hiding]\n",
            "src=[Не бев убеден], target=[I wasnt convinced], predicted=[i wasnt convinced]\n",
            "src=[Не ми оди тенис], target=[Im bad at tennis], predicted=[im bad at tennis]\n",
            "src=[Том е експерт], target=[Toms an expert], predicted=[toms an expert]\n",
            "src=[Пробај да седиш мирен], target=[Try to stand still], predicted=[try to stand still]\n",
            "src=[Том е несопирлив], target=[Tom is unstoppable], predicted=[tom is]\n",
            "src=[Користи акрилни бои], target=[Use acrylic paint], predicted=[use acrylic paint]\n",
            "src=[Ќе спијам], target=[Im going to sleep], predicted=[im going to sleep]\n",
            "src=[Мене тоа не би ми пречело], target=[Id be OK with that], predicted=[id be ok with that]\n",
            "src=[Ќе бидам тука], target=[Ill be right here], predicted=[ill be right here]\n",
            "src=[Том беше заинтересиран], target=[Tom was interested], predicted=[tom was interested]\n",
            "src=[Ми студи и гладен сум], target=[Im cold and hungry], predicted=[im cold and hungry]\n",
            "src=[Се расчистив со него], target=[I got even with him], predicted=[i got even with him]\n",
            "src=[Том е доста мрзлив], target=[Tom is quite lazy], predicted=[tom is quite lazy]\n",
            "src=[Том е чуден], target=[Tom is odd], predicted=[tom is odd]\n",
            "src=[Бирово е мое], target=[This desk is mine], predicted=[this desk is mine]\n",
            "src=[Ние сме месари], target=[Were butchers], predicted=[were butchers]\n",
            "src=[Нели е тоа необично], target=[Isnt that curious], predicted=[isnt that curious]\n",
            "src=[Не си лузер], target=[Youre not a loser], predicted=[youre not a loser]\n",
            "src=[Тоа е ѓубре], target=[Its garbage], predicted=[its garbage]\n",
            "src=[Том не ми кажа], target=[Tom didnt tell me], predicted=[tom didnt tell me]\n",
            "src=[Ме боли колкот], target=[My hip hurts], predicted=[my hip hurts]\n",
            "src=[Само се шалев], target=[I was just kidding], predicted=[i was just kidding]\n",
            "src=[Не бев сам тогаш], target=[I wasnt alone then], predicted=[i wasnt alone then]\n",
            "src=[Сите го знаат тоа], target=[Everyone knows that], predicted=[everyone knows that]\n",
            "src=[Не шпекулирам], target=[Im not speculating], predicted=[im not speculating]\n",
            "src=[Мразам да ризикувам], target=[I hate taking risks], predicted=[i hate taking risks]\n",
            "src=[Том изгледа осамено], target=[Tom seems lonely], predicted=[tom looks lonely]\n",
            "src=[Том сака да работи], target=[Tom wants to work], predicted=[tom wants to work]\n",
            "src=[Дај да се испијаниме], target=[Lets get drunk], predicted=[lets get drunk]\n",
            "src=[Можно е да успее], target=[It just might work], predicted=[it just might work]\n",
            "src=[Тоа ли е сѐ], target=[Is that all], predicted=[is that all]\n",
            "src=[Го прашав Том], target=[I asked Tom], predicted=[i asked tom]\n",
            "src=[Ја имаш нашата понуда], target=[You have our offer], predicted=[you have our offer]\n",
            "src=[Тој си ја врши должноста], target=[He does his duty], predicted=[he does his duty]\n",
            "src=[Возот застана], target=[The train stopped], predicted=[the train stopped]\n",
            "src=[Се е бело], target=[Everything is white], predicted=[everything is white]\n",
            "src=[Не можам да ги кривам], target=[I cant blame them], predicted=[i cant blame them]\n",
            "src=[Не се сомневам дека си во право], target=[I bet youre right], predicted=[i bet youre right]\n",
            "BLEU-1: 0.676180\n",
            "BLEU-2: 0.606042\n",
            "BLEU-3: 0.537986\n",
            "BLEU-4: 0.323903\n",
            "test\n",
            "src=[Том збесна], target=[Tom got furious], predicted=[tom became furious]\n",
            "src=[Можеме да се обидеме], target=[We can try], predicted=[we can try]\n",
            "src=[Ќе биде ли Том спремен], target=[Will Tom be ready], predicted=[will tom be ready]\n",
            "src=[Треба да тргам], target=[I should get going], predicted=[i should get going]\n",
            "src=[Слави ли Том], target=[Is Tom celebrating], predicted=[is tom celebrating]\n",
            "src=[Том сите ги нервира], target=[Tom bugs everyone], predicted=[tom bugs everyone]\n",
            "src=[Влегувај внатре], target=[Get inside], predicted=[get inside]\n",
            "src=[Слушнавме пукот], target=[We heard a gunshot], predicted=[we heard a gunshot]\n",
            "src=[Донеси му вода на Том], target=[Get Tom some water], predicted=[get tom some water]\n",
            "src=[Тој ми е стар пријател], target=[Hes my old friend], predicted=[hes my old old]\n",
            "src=[Прилично сум начукан], target=[Im pretty zonked], predicted=[im pretty zonked]\n",
            "src=[Том е нерешителен], target=[Tom is indecisive], predicted=[tom is indecisive]\n",
            "src=[Има ли некој таму], target=[Is someone in there], predicted=[is anyone in there]\n",
            "src=[И двајцата те мразиме], target=[We both hate you], predicted=[we both hate you]\n",
            "src=[Том има мечта], target=[Toms imaginative], predicted=[toms imaginative]\n",
            "src=[Фин си], target=[Youre nice], predicted=[youre nice]\n",
            "src=[Насмеј се], target=[Smile], predicted=[smile]\n",
            "src=[Том се однесува откачено], target=[Tom is acting nuts], predicted=[tom is acting nuts]\n",
            "src=[Како беше концертот], target=[How was the concert], predicted=[how was the concert]\n",
            "src=[Том влезе во таксито], target=[Tom got in the cab], predicted=[tom got in the cab]\n",
            "src=[Тргај ми се од патот], target=[Keep out of my way], predicted=[move out of my way]\n",
            "src=[Том простенка], target=[Tom groaned], predicted=[tom groaned]\n",
            "src=[Ќе погледаме], target=[Well check it out], predicted=[well check it out]\n",
            "src=[Алармот одѕвони], target=[The alarm sounded], predicted=[the alarm sounded]\n",
            "src=[Том звучеше вознемирено], target=[Tom sounded upset], predicted=[tom sounded upset]\n",
            "src=[Прва година сум], target=[Im just a freshman], predicted=[im just a freshman]\n",
            "src=[Угаси го огнот], target=[Put that fire out], predicted=[put that fire out]\n",
            "src=[Пушти ме да продолжам], target=[Let me continue], predicted=[let me continue]\n",
            "src=[Том е среќен], target=[Toms glad], predicted=[toms glad]\n",
            "src=[Не можеш да победиш Том], target=[You cant win Tom], predicted=[you cant win tom]\n",
            "src=[Отворени ли ти се очите], target=[Are your eyes open], predicted=[are your eyes open]\n",
            "src=[Отворено е], target=[Its open], predicted=[its open]\n",
            "src=[Зарем не е прекрасно], target=[Isnt it gorgeous], predicted=[isnt it gorgeous]\n",
            "src=[Само ќе си седам тука], target=[Ill just sit here], predicted=[ill just sit here]\n",
            "src=[Само го кренав], target=[I just picked it up], predicted=[i just picked it up]\n",
            "src=[Нормално], target=[Of course], predicted=[of course]\n",
            "src=[Буден сум], target=[Im awake], predicted=[im awake]\n",
            "src=[Нема да постапат така], target=[They wont do that], predicted=[they wont do that]\n",
            "src=[Том е преоптеретен], target=[Tom is overwhelmed], predicted=[tom is overwhelmed]\n",
            "src=[Не сум неписмен], target=[Im not illiterate], predicted=[im not illiterate]\n",
            "src=[Сите го направија тоа], target=[Everyone did it], predicted=[everyone did it]\n",
            "src=[Ќе наминам попосле], target=[Ill stop by later], predicted=[ill stop by later]\n",
            "src=[Штотуку видов дух], target=[I just saw a ghost], predicted=[i just saw a ghost]\n",
            "src=[Никого не гледам], target=[I dont see anyone], predicted=[i dont see anyone]\n",
            "src=[Том изгледа сомнително], target=[Tom looks dubious], predicted=[tom looks dubious]\n",
            "src=[Том изгледа неспокојно], target=[Tom looks restless], predicted=[tom looks restless]\n",
            "src=[Том е секогаш љубезен], target=[Tom is always nice], predicted=[tom is always nice]\n",
            "src=[Том е специфичен], target=[Tom is particular], predicted=[tom is particular]\n",
            "src=[Не бев организиран], target=[I wasnt organized], predicted=[i wasnt organized]\n",
            "src=[Том изгледа немирно], target=[Tom looks restless], predicted=[tom looks restless]\n",
            "src=[Станав околу пет], target=[I got up about five], predicted=[i got up about five]\n",
            "src=[Нема сомнеж], target=[There is no doubt], predicted=[its a sure thing]\n",
            "src=[Зошто се плаши Том], target=[Why is Tom scared], predicted=[why is tom scared]\n",
            "src=[Том многу сака лалиња], target=[Tom loves tulips], predicted=[tom loves tulips]\n",
            "src=[Одам јас во теретана], target=[Im off to the gym], predicted=[im off to the gym]\n",
            "src=[Том е возлен], target=[Toms furious], predicted=[toms furious]\n",
            "src=[Леле во право си], target=[Wow youre right], predicted=[wow youre right]\n",
            "src=[Том има три коли], target=[Tom has three cars], predicted=[tom has three cars]\n",
            "src=[Мило ми е што се сложуваш], target=[Im glad you agree], predicted=[im glad you agree]\n",
            "src=[Дај ми чепкалка], target=[Give me a toothpick], predicted=[give me a toothpick]\n",
            "src=[Инвестиција е], target=[Its an investment], predicted=[its an investment]\n",
            "src=[Смее ли да останам], target=[Can I stay], predicted=[can i stay]\n",
            "src=[Го оставам Том да спие до подоцна], target=[I let Tom sleep in], predicted=[i let tom sleep in]\n",
            "src=[Ете го докторот], target=[Theres the doctor], predicted=[theres the doctor]\n",
            "src=[Таа се прослави], target=[She became famous], predicted=[she became famous]\n",
            "src=[Едвај стојам на нозе], target=[I can hardly stand], predicted=[i can hardly stand]\n",
            "src=[Немаш одбрана], target=[Youre defenseless], predicted=[youre defenseless]\n",
            "src=[Само одбери еден], target=[Just pick one out], predicted=[just pick one out]\n",
            "src=[Немаме одбрана], target=[Were defenseless], predicted=[were defenseless]\n",
            "src=[Не се согласувам со тебе], target=[I disagree with you], predicted=[i disagree with you]\n",
            "src=[Ти ли си тоа], target=[Is it you], predicted=[is that you]\n",
            "src=[Кој престана], target=[Who stopped], predicted=[who stopped]\n",
            "src=[Кој ти помогнал], target=[Who helped you], predicted=[who helped you]\n",
            "src=[Те спасив], target=[I saved you], predicted=[i saved you]\n",
            "src=[Сакам прошетки], target=[I like taking walks], predicted=[i like taking walks]\n",
            "src=[Пардон], target=[Excuse me], predicted=[excuse me]\n",
            "src=[Заклучи си ја вратата], target=[Lock your door], predicted=[lock your door]\n",
            "src=[Тој обожава да озборува], target=[He loves to gossip], predicted=[he loves to gossip]\n",
            "src=[Кој те изненади], target=[Who surprised you], predicted=[who surprised you]\n",
            "src=[Ништо не им кажав], target=[I told them nothing], predicted=[i told them nothing]\n",
            "src=[Том не се појави], target=[Tom didnt turn up], predicted=[tom didnt turn up]\n",
            "src=[Имав ли избор], target=[Did I have a choice], predicted=[did i have a choice]\n",
            "src=[Татко ми има работа], target=[My father is busy], predicted=[my father is busy]\n",
            "src=[Не се малоумни], target=[Theyre not stupid], predicted=[theyre not stupid]\n",
            "src=[Том ми е партнер], target=[Tom is my partner], predicted=[tom is my partner]\n",
            "src=[Том е сѐ уште зашеметен], target=[Tom is still woozy], predicted=[tom is still woozy]\n",
            "src=[Том го извади капакот], target=[Tom opened the lid], predicted=[tom opened the lid]\n",
            "src=[Сега сме пријатели], target=[Were friends now], predicted=[were friends now]\n",
            "src=[Може ли и за мене], target=[Can I have one too], predicted=[can i have one too]\n",
            "src=[Том е црнокос], target=[Tom has black hair], predicted=[tom has black hair]\n",
            "src=[Се спремав], target=[Ive been preparing], predicted=[ive been preparing]\n",
            "src=[Не сакам да трчам], target=[I dont like to run], predicted=[i dont like to run]\n",
            "src=[Том си ги врза чевлите], target=[Tom tied his shoes], predicted=[tom tied his shoes]\n",
            "src=[Прифаќам подароци], target=[I accept gifts], predicted=[i accept gifts]\n",
            "src=[Тоа е навистина добро], target=[Thats really good], predicted=[thats really good]\n",
            "src=[Добра приказна е], target=[Its a good story], predicted=[its a good story]\n",
            "src=[Ќе нарачам пица], target=[Im ordering pizza], predicted=[im ordering pizza]\n",
            "src=[Том штотуку предложи брак], target=[Tom just proposed], predicted=[tom just proposed]\n",
            "src=[Ни требаат парите], target=[We need the money], predicted=[we need the money]\n",
            "src=[Том е многу суетен], target=[Tom has a huge ego], predicted=[tom has a huge]\n",
            "BLEU-1: 0.655508\n",
            "BLEU-2: 0.583817\n",
            "BLEU-3: 0.518358\n",
            "BLEU-4: 0.309730\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}